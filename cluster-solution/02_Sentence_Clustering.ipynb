{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "inputWidgets": {},
          "nuid": "9136aa06-3fe6-4d0c-a724-fcd86d7cca4c",
          "showTitle": false,
          "title": ""
        },
        "id": "-ZSSACNzdXBj"
      },
      "source": [
        "In this notebook, SentenceTransformer is used to embed recommendation texts into vectors. Then AgglomerativeClustering is used to put similar recommendations into groups."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "inputWidgets": {},
          "nuid": "8b0b319f-745c-4c9e-9c5a-2e28c630eec3",
          "showTitle": false,
          "title": ""
        },
        "id": "ky7Dfn7IdXBo",
        "outputId": "d7e44915-16e7-460f-f3ac-676c2ede9cb1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.4.0/en_core_web_sm-3.4.0.tar.gz\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.4.0/en_core_web_sm-3.4.0.tar.gz (12.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m25.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: spacy<3.5.0,>=3.4.0 in /usr/local/lib/python3.9/dist-packages (from en-core-web-sm==3.4.0) (3.4.4)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.9/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (2.27.1)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.9.1 in /usr/local/lib/python3.9/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (0.10.1)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.9/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (3.3.0)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in /usr/local/lib/python3.9/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (1.10.7)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.9/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (23.0)\n",
            "Requirement already satisfied: typer<0.8.0,>=0.3.0 in /usr/local/lib/python3.9/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (0.7.0)\n",
            "Requirement already satisfied: pathy>=0.3.5 in /usr/local/lib/python3.9/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (0.10.1)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.9/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (1.0.4)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.9/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (1.22.4)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.9/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (2.0.8)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.9/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (67.6.1)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.10 in /usr/local/lib/python3.9/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (3.0.12)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.9/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (3.1.2)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.9/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (1.0.9)\n",
            "Requirement already satisfied: thinc<8.2.0,>=8.1.0 in /usr/local/lib/python3.9/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (8.1.9)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.9/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (2.0.7)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.9/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (3.0.8)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.9/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (6.3.0)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.9/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (2.4.6)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.9/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (4.65.0)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.9/dist-packages (from pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (4.5.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (1.26.15)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (3.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (2022.12.7)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.9/dist-packages (from thinc<8.2.0,>=8.1.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (0.0.4)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.9/dist-packages (from thinc<8.2.0,>=8.1.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (0.7.9)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.9/dist-packages (from typer<0.8.0,>=0.3.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (8.1.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.9/dist-packages (from jinja2->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (2.1.2)\n",
            "fatal: destination path 'all-mpnet-base-v2' already exists and is not an empty directory.\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting sentence_transformers\n",
            "  Downloading sentence-transformers-2.2.2.tar.gz (85 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.0/86.0 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting transformers<5.0.0,>=4.6.0\n",
            "  Downloading transformers-4.27.4-py3-none-any.whl (6.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.8/6.8 MB\u001b[0m \u001b[31m56.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.9/dist-packages (from sentence_transformers) (4.65.0)\n",
            "Requirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.9/dist-packages (from sentence_transformers) (2.0.0+cu118)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.9/dist-packages (from sentence_transformers) (0.15.1+cu118)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.9/dist-packages (from sentence_transformers) (1.22.4)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.9/dist-packages (from sentence_transformers) (1.2.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.9/dist-packages (from sentence_transformers) (1.10.1)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.9/dist-packages (from sentence_transformers) (3.8.1)\n",
            "Collecting sentencepiece\n",
            "  Downloading sentencepiece-0.1.98-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m70.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting huggingface-hub>=0.4.0\n",
            "  Downloading huggingface_hub-0.13.4-py3-none-any.whl (200 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m200.1/200.1 kB\u001b[0m \u001b[31m22.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.9/dist-packages (from huggingface-hub>=0.4.0->sentence_transformers) (6.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from huggingface-hub>=0.4.0->sentence_transformers) (2.27.1)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.9/dist-packages (from huggingface-hub>=0.4.0->sentence_transformers) (23.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.9/dist-packages (from huggingface-hub>=0.4.0->sentence_transformers) (4.5.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from huggingface-hub>=0.4.0->sentence_transformers) (3.11.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.9/dist-packages (from torch>=1.6.0->sentence_transformers) (3.1.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.9/dist-packages (from torch>=1.6.0->sentence_transformers) (3.1)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.9/dist-packages (from torch>=1.6.0->sentence_transformers) (2.0.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.9/dist-packages (from torch>=1.6.0->sentence_transformers) (1.11.1)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.9/dist-packages (from triton==2.0.0->torch>=1.6.0->sentence_transformers) (3.25.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.9/dist-packages (from triton==2.0.0->torch>=1.6.0->sentence_transformers) (16.0.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.9/dist-packages (from transformers<5.0.0,>=4.6.0->sentence_transformers) (2022.10.31)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1\n",
            "  Downloading tokenizers-0.13.3-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m89.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: click in /usr/local/lib/python3.9/dist-packages (from nltk->sentence_transformers) (8.1.3)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.9/dist-packages (from nltk->sentence_transformers) (1.2.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.9/dist-packages (from scikit-learn->sentence_transformers) (3.1.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.9/dist-packages (from torchvision->sentence_transformers) (8.4.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.9/dist-packages (from jinja2->torch>=1.6.0->sentence_transformers) (2.1.2)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests->huggingface-hub>=0.4.0->sentence_transformers) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests->huggingface-hub>=0.4.0->sentence_transformers) (3.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests->huggingface-hub>=0.4.0->sentence_transformers) (2022.12.7)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->huggingface-hub>=0.4.0->sentence_transformers) (1.26.15)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.9/dist-packages (from sympy->torch>=1.6.0->sentence_transformers) (1.3.0)\n",
            "Building wheels for collected packages: sentence_transformers\n",
            "  Building wheel for sentence_transformers (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sentence_transformers: filename=sentence_transformers-2.2.2-py3-none-any.whl size=125942 sha256=6ab5b6f5529cd1517fdad725c4c54f973824e06d9d25003c2575b060dd653176\n",
            "  Stored in directory: /root/.cache/pip/wheels/71/67/06/162a3760c40d74dd40bc855d527008d26341c2b0ecf3e8e11f\n",
            "Successfully built sentence_transformers\n",
            "Installing collected packages: tokenizers, sentencepiece, huggingface-hub, transformers, sentence_transformers\n",
            "Successfully installed huggingface-hub-0.13.4 sentence_transformers-2.2.2 sentencepiece-0.1.98 tokenizers-0.13.3 transformers-4.27.4\n"
          ]
        }
      ],
      "source": [
        "!pip --trusted-host github.com --trusted-host objects.githubusercontent.com install https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.4.0/en_core_web_sm-3.4.0.tar.gz\n",
        "!git -c http.sslVerify=false clone https://huggingface.co/sentence-transformers/all-mpnet-base-v2\n",
        "!pip install sentence_transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "inputWidgets": {},
          "nuid": "2c24b22c-6fbc-4289-95ef-31c8876127d4",
          "showTitle": false,
          "title": ""
        },
        "id": "Q-sQF00AdXBq"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import sys\n",
        "module_path = os.path.abspath(os.path.join('..'))\n",
        "if module_path not in sys.path:\n",
        "    sys.path.append(module_path)\n",
        "    \n",
        "from src.data_clean_functions import *"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "inputWidgets": {},
          "nuid": "f005d434-4384-4f82-9c5f-7087a6c7eef0",
          "showTitle": false,
          "title": ""
        },
        "id": "XP7IL0p0dXBr"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import spacy\n",
        "import re\n",
        "from sklearn.cluster import AgglomerativeClustering\n",
        "from matplotlib import pyplot as plt\n",
        "from scipy.cluster.hierarchy import dendrogram\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import pickle\n",
        "#nlp = spacy.load(\"en_core_web_sm\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "inputWidgets": {},
          "nuid": "93df12ba-c503-4442-97a4-7676cd99cb84",
          "showTitle": false,
          "title": ""
        },
        "id": "LVGPVDjudXBr",
        "outputId": "5234919f-cef3-46eb-914c-8dbfee084e52",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['1. establish a separate budget line item for the advisory committee on minonty veterans.', '2. appropriate 125,000.00 for the fiscal year year budget for the advisory committee on minority veterans.', '3. recognize the appropnate regional and national minority organizations that serve our countrys diverse population proups and invite them as genuine partners, consultants, advisors and expert witnesses in enriching the departments commitment to serve the needs of our minomty veterans.', '4. authonze an assessment be conducted and a report submitted in fy 97 by the department of veterans affairs to determine the validity of the commuttees concern regarding ethnic representation on the rating panels and the ment review boards, as well as among the counselors who advise our minority veteran populations.', '5. establish and fund an intra agency task force, chaired by the director of the center for minonty veterans, to conduct a needs assessment for health care, benefits and compensation research of all our minonty veterans by ethnic groups and submit a report by monthyear,.']\n"
          ]
        }
      ],
      "source": [
        "# load recommendation lists\n",
        "recommd = pd.read_csv('CMV_Reports_Article_Recommendations_lists.csv')\n",
        "recommd['RecommendLists'] = [i.split(\"&&\") for i in recommd['Recommendations']]\n",
        "print(recommd['RecommendLists'][0][:5])\n",
        "# print(recommd.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "inputWidgets": {},
          "nuid": "04024f16-a257-473e-9e1e-8f34dfbefba5",
          "showTitle": false,
          "title": ""
        },
        "id": "NbUDnW1ldXBs",
        "outputId": "1c904964-8f48-4d67-f005-a450ed1ac05d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "259\n",
            "11926\n"
          ]
        }
      ],
      "source": [
        "# check the number of recommendations and number of words in total\n",
        "i = 0\n",
        "j = 0\n",
        "for red in recommd['RecommendLists']:\n",
        "        i = i + len(red)\n",
        "        for r in red:\n",
        "#             if len(r.split(\" \")) > 1143:\n",
        "#                 print(r)\n",
        "            j = j + len(r.split(\" \"))\n",
        "        \n",
        "print(i)\n",
        "print(j)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "inputWidgets": {},
          "nuid": "a83efa3b-d4fc-494c-ba48-6b986cd6434b",
          "showTitle": false,
          "title": ""
        },
        "id": "qQKj6lSjdXBt"
      },
      "source": [
        "Use sentence encoders to embed recommendations into vectors. https://www.sbert.net/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "inputWidgets": {},
          "nuid": "5d50893f-7b5c-4547-b58b-439bc209e294",
          "showTitle": false,
          "title": ""
        },
        "id": "15S7eFyydXBt"
      },
      "outputs": [],
      "source": [
        "# import tensorflow_hub as hub\n",
        "# from sklearn.manifold import TSNE\n",
        "\n",
        "# def embbedfn(text):\n",
        "#     embed = hub.load(\"https://tfhub.dev/google/universal-sentence-encoder/4\")\n",
        "# #     strsent = list(nlp(text).sents)\n",
        "# #     strsent =  [str(onesentence) for onesentence in strsent if len(onesentence)>10]\n",
        "#     embeddings = embed(text)\n",
        "# #     mapped_embeddings = TSNE(n_components=2, metric='cosine', init='pca', verbose = 1).fit_transform(embeddingslocal)\n",
        "#     return embeddings\n",
        "\n",
        "def embbedbert(text, model):\n",
        "    embeddings = model.encode(text)\n",
        "    return embeddings\n",
        "\n",
        "model = SentenceTransformer('all-mpnet-base-v2')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "inputWidgets": {},
          "nuid": "18c4ac4c-b9a7-4d49-af27-f11b0d797b29",
          "showTitle": false,
          "title": ""
        },
        "id": "8NJdDFt-dXBu"
      },
      "outputs": [],
      "source": [
        "# bert sentence encoder\n",
        "recommd['embeddings'] = recommd.RecommendLists.apply(lambda x: embbedbert(x, model))\n",
        "recommd['embeddings'][0].shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "inputWidgets": {},
          "nuid": "75d67d7a-04e2-4890-b0be-e8ebe90ad0b6",
          "showTitle": false,
          "title": ""
        },
        "id": "IA5zJu_GdXBv"
      },
      "outputs": [],
      "source": [
        "# unlist the nested lists\n",
        "# previous structure -> [[recommd1 for 1996, recommd2 for 1996, ...], [recommd1 for 1997, recommd2 for 1997, ...],...]\n",
        "# current structure -> [recommd1 for 1996, recommd2 for 1996, ..., recommd1 for 1997, recommd2 for 1997, ...]\n",
        "embeddings = []\n",
        "for i in recommd['embeddings']:\n",
        "    embeddings.extend(i)\n",
        "RecommendLists = []\n",
        "for i in recommd['RecommendLists']:\n",
        "    RecommendLists.extend(i)\n",
        "    \n",
        "print(len(embeddings))\n",
        "print(len(RecommendLists))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "inputWidgets": {},
          "nuid": "6aa7d497-3500-42d5-9c73-53c1eedd0fd7",
          "showTitle": false,
          "title": ""
        },
        "id": "AWhTLJkFdXBv"
      },
      "outputs": [],
      "source": [
        "Years = []\n",
        "for i in range(len(recommd['RecommendLists'])):\n",
        "    Years.extend(list(np.repeat(recommd['Year'][i], len(recommd['RecommendLists'][i]))))\n",
        "print(len(Years))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "inputWidgets": {},
          "nuid": "b27f96bd-86f9-4cae-acef-d3d3253485a4",
          "showTitle": false,
          "title": ""
        },
        "id": "RIANA-kBdXBw"
      },
      "outputs": [],
      "source": [
        "from sklearn import metrics\n",
        "from sklearn.metrics import davies_bouldin_score\n",
        "# davies_bouldin_score: ratio of within-cluster distances to between-cluster distances\n",
        "# clusters which are farther apart and less dispersed will result in a better score."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "inputWidgets": {},
          "nuid": "414516d2-73e3-4c4b-946e-8dc9677eedbb",
          "showTitle": false,
          "title": ""
        },
        "id": "TVpgM-IfdXBw"
      },
      "source": [
        "Start parameter tuning for classification. There are many metrics used here for measuring classification performance. \n",
        "1. davies_bouldin_score: ratio of within-cluster distances to between-cluster distances. \n",
        "2. silhouette_score: The best value is 1 and the worst value is -1. Values near 0 indicate overlapping clusters. Negative values generally indicate that a sample has been assigned to the wrong cluster, as a different cluster is more similar.\n",
        "3. calinski_harabasz_score: It is also known as the Variance Ratio Criterion. Ratio of the sum of between-cluster dispersion and of within-cluster dispersion."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "inputWidgets": {},
          "nuid": "dbdf0df5-493c-44a3-8fcc-92cebe54db60",
          "showTitle": false,
          "title": ""
        },
        "id": "sSGz4yNjdXBx"
      },
      "outputs": [],
      "source": [
        "# parameter tunings\n",
        "# final output is a dataframe that stores all the parameters and their corresponding classification scores\n",
        "\n",
        "def agg_cluster_tuning(grid):\n",
        "    output = {'affinity': [],\n",
        "             'linkage': [],\n",
        "              'distance_threshold': [],\n",
        "             'silhouette_score': [],\n",
        "             'calinski_harabasz_score': [],\n",
        "             'davies_bouldin_score': [],\n",
        "             'number_of_clusters': []}\n",
        "    for aff in grid['affinity']:\n",
        "        for link in grid['linkage']:\n",
        "            if link == 'ward' and aff != 'euclidean':\n",
        "                continue\n",
        "            clusterer = AgglomerativeClustering(n_clusters=None, affinity=aff, linkage=link, distance_threshold=0.2)\n",
        "            clusterer.fit_predict(embeddings)\n",
        "            dist = clusterer.distances_\n",
        "            minD = min(dist)\n",
        "            maxD = max(dist)\n",
        "            step = (max(dist)-min(dist))/10\n",
        "            for d in np.arange(minD+step, maxD, step):\n",
        "#                 print(d)\n",
        "                # fit\n",
        "                clusterer = AgglomerativeClustering(n_clusters=None, affinity=aff, linkage=link, distance_threshold=d)\n",
        "                clusters = clusterer.fit_predict(embeddings)\n",
        "                output['affinity'].append(aff)\n",
        "                output['linkage'].append(link)\n",
        "                try:\n",
        "                    output['silhouette_score'].append(metrics.silhouette_score(embeddings, clusters, metric='euclidean'))\n",
        "                except:\n",
        "                    output['silhouette_score'].append(0)\n",
        "                try:\n",
        "                    output['calinski_harabasz_score'].append(metrics.calinski_harabasz_score(embeddings, clusters))\n",
        "                except:\n",
        "                    output['calinski_harabasz_score'].append(0)\n",
        "                try:\n",
        "                    output['davies_bouldin_score'].append(davies_bouldin_score(embeddings, clusters))\n",
        "                except:\n",
        "                    output['davies_bouldin_score'].append(0)\n",
        "                \n",
        "                output['distance_threshold'].append(d)\n",
        "                output['number_of_clusters'].append(max(clusters)+1)\n",
        "\n",
        "    return output\n",
        "         "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "inputWidgets": {},
          "nuid": "06e02673-aabd-42cb-a84b-9a5547f7f571",
          "showTitle": false,
          "title": ""
        },
        "id": "6MKvvtsedXBx"
      },
      "outputs": [],
      "source": [
        "grid = {'affinity': ['euclidean', 'manhattan', 'l1', 'l2', 'cosine'],\n",
        "           'linkage': ['ward', 'complete', 'average', 'single']}\n",
        "# get the output dataframe \n",
        "result = agg_cluster_tuning(grid)\n",
        "output = pd.DataFrame(result)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "inputWidgets": {},
          "nuid": "5f81436c-e0eb-4e69-928f-29da90ebf67d",
          "showTitle": false,
          "title": ""
        },
        "id": "ocMOQ6VVdXBx"
      },
      "outputs": [],
      "source": [
        "import datetime as dt\n",
        "batch = dt.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "batch \n",
        "output['batch'] = batch \n",
        "display(output)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "inputWidgets": {},
          "nuid": "db328c92-0bed-48ee-8b1a-bcbf532c1a44",
          "showTitle": false,
          "title": ""
        },
        "id": "9v-J2FMUdXBy"
      },
      "outputs": [],
      "source": [
        "# save output\n",
        "with open('recommendation_parameter_tuning', 'wb') as fp:\n",
        "    pickle.dump(output, fp)\n",
        "# with open('/dbfs/NAII/CMV Reports/recommendation_parameter_tuning', 'rb') as fp:\n",
        "#     output = pickle.load(fp)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "inputWidgets": {},
          "nuid": "1337e9e7-ad41-4aba-83d3-3271b7756642",
          "showTitle": false,
          "title": ""
        },
        "id": "GEOmwkD-dXBy"
      },
      "outputs": [],
      "source": [
        "def plot_dendrogram(model, **kwargs):\n",
        "    # Create linkage matrix and then plot the dendrogram\n",
        "\n",
        "    # create the counts of samples under each node\n",
        "    counts = np.zeros(model.children_.shape[0])\n",
        "    n_samples = len(model.labels_)\n",
        "    for i, merge in enumerate(model.children_):\n",
        "        current_count = 0\n",
        "        for child_idx in merge:\n",
        "            if child_idx < n_samples:\n",
        "                current_count += 1  # leaf node\n",
        "            else:\n",
        "                current_count += counts[child_idx - n_samples]\n",
        "        counts[i] = current_count\n",
        "\n",
        "    linkage_matrix = np.column_stack([model.children_, model.distances_,\n",
        "                                      counts]).astype(float)\n",
        "\n",
        "    # Plot the corresponding dendrogram\n",
        "    dendrogram(linkage_matrix, **kwargs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "inputWidgets": {},
          "nuid": "45c47ef2-bd16-4a29-a1eb-97a07d49bd69",
          "showTitle": false,
          "title": ""
        },
        "id": "l4BSzv-JdXBy"
      },
      "source": [
        "In sum, calinski harabasz score and the number of clusters were used to make the final decision on the values for the parameters. I chose one combination that will have relative good calinski_harabasz_score and reasonable number of clusters. silhouette_score and davies_bouldin_score are used to provide extra informantion on the classification performance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "inputWidgets": {},
          "nuid": "4de1fd73-d4b0-4627-a4c5-a4d83259d5be",
          "showTitle": false,
          "title": ""
        },
        "id": "RL7U3YMidXBz"
      },
      "outputs": [],
      "source": [
        "clusterer = AgglomerativeClustering(n_clusters=None, affinity=\"euclidean\", linkage=\"ward\", distance_threshold = 1.25)\n",
        "clusters = clusterer.fit_predict(embeddings)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "inputWidgets": {},
          "nuid": "e40b5712-a218-407c-a37c-a391009e3eed",
          "showTitle": false,
          "title": ""
        },
        "id": "TGlnfk7IdXB0"
      },
      "outputs": [],
      "source": [
        "# plot hierarchical clustering dendrogram\n",
        "plt.title('Hierarchical Clustering Dendrogram')\n",
        "# plot the top three levels of the dendrogram\n",
        "plot_dendrogram(clusterer, truncate_mode='level', p=6)\n",
        "plt.xlabel(\"Number of points in node (or index of point if no parenthesis).\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "inputWidgets": {},
          "nuid": "d3026d29-39cc-487f-9cb4-08c31f1b8907",
          "showTitle": false,
          "title": ""
        },
        "id": "5qJGlTGGdXB0"
      },
      "outputs": [],
      "source": [
        "output.to_csv('CMV_Reports_Article_Recommendations_clusters.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "inputWidgets": {},
          "nuid": "760c6c5a-7b96-4c80-9d95-eb6f39fb6db7",
          "showTitle": false,
          "title": ""
        },
        "id": "DAmXMYcTdXB0"
      },
      "outputs": [],
      "source": [
        "# plot of clusters in 2D\n",
        "from sklearn.manifold import TSNE\n",
        "from collections import Counter\n",
        "df = pd.DataFrame(embeddings)\n",
        "mapped_embeddings = TSNE(n_components=2, metric='euclidean', init='pca').fit_transform(df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "inputWidgets": {},
          "nuid": "3e973c60-2990-4ae8-95d5-6ce918410365",
          "showTitle": false,
          "title": ""
        },
        "id": "iMv5zpWydXB0"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(15,10))\n",
        "x = mapped_embeddings[:,0]\n",
        "y = mapped_embeddings[:,1]\n",
        "ax.scatter(x, y, c = clusters, cmap = 'hsv')\n",
        "# ax.legend(clusters, title='clusters')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "inputWidgets": {},
          "nuid": "837fbe62-b80c-4b3b-acf5-23ea4695a71a",
          "showTitle": false,
          "title": ""
        },
        "id": "qU7j950DdXB1"
      },
      "outputs": [],
      "source": [
        "# present the distribution of clusters: number of recommendations in each cluster\n",
        "display(pd.DataFrame({'cluster': [i for (i,j) in Counter(clusters).most_common()], 'freq': [j for (i,j) in Counter(clusters).most_common()]}))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "inputWidgets": {},
          "nuid": "3d469ba8-4bbf-4085-a702-6dbc1c507a9e",
          "showTitle": false,
          "title": ""
        },
        "id": "gyccJYn9dXB1"
      },
      "outputs": [],
      "source": [
        "# create the output table with 3 columns: recommendation text, clusters and years\n",
        "recommd_cluster = pd.DataFrame({\"text\": RecommendLists, \"cluster\": clusters, 'Years': Years})\n",
        "display(recommd_cluster.loc[recommd_cluster['cluster'] == 5])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "inputWidgets": {},
          "nuid": "a652d000-6764-4b7b-b535-c60e23867c49",
          "showTitle": false,
          "title": ""
        },
        "id": "2bbjR5KgdXB1"
      },
      "outputs": [],
      "source": [
        "# save recommd_cluster\n",
        "with open('recommendation_clusters', 'wb') as fp:\n",
        "    pickle.dump(recommd_cluster, fp)\n",
        "# with open('/dbfs/NAII/CMV Reports/recommendation_clusters', 'rb') as fp:\n",
        "#     recommd_cluster = pickle.load(fp)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "inputWidgets": {},
          "nuid": "cf31d53a-5fe4-4dd7-adbb-dc928b059a49",
          "showTitle": false,
          "title": ""
        },
        "id": "OgNgIr8DdXB1"
      },
      "outputs": [],
      "source": [
        "import plotly.express as px\n",
        "\n",
        "recommd_cluster['cluster'] = recommd_cluster['cluster'].astype(str)\n",
        "fig = px.scatter(recommd_cluster, y=\"Years\", x=\"cluster\", color=\"cluster\")\n",
        "\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "inputWidgets": {},
          "nuid": "ccd98751-a4b7-44f1-b633-f1aeb95140d9",
          "showTitle": false,
          "title": ""
        },
        "id": "Oq3YJsFOdXB2"
      },
      "source": [
        "Generate recommendation summary text for each cluster"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "inputWidgets": {},
          "nuid": "beb04c02-3c3f-436e-a9cb-5fff93efb508",
          "showTitle": false,
          "title": ""
        },
        "id": "xxMCCByRdXB2"
      },
      "outputs": [],
      "source": [
        "# first concatenate all the text strings from the same cluster\n",
        "dfCluster = recommd_cluster[['cluster', 'text']].groupby('cluster')['text'].apply(' '.join).reset_index()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "inputWidgets": {},
          "nuid": "76809622-0b27-439e-8559-ba41eee29e93",
          "showTitle": false,
          "title": ""
        },
        "id": "ovhuGEQMdXB2"
      },
      "outputs": [],
      "source": [
        "# use Bart large cnn model for text summarization\n",
        "import os\n",
        "import sys\n",
        "import certifi\n",
        "from transformers import AutoTokenizer, BartForConditionalGeneration\n",
        "\n",
        "model = BartForConditionalGeneration.from_pretrained(\"facebook/bart-large-cnn\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"facebook/bart-large-cnn\")\n",
        "#os.environ['REQUESTS_CA_BUNDLE'] = os.path.join(os.path.dirname(sys.argv[0]), certifi.where())\n",
        "\n",
        "def allsummary(body):\n",
        "    inputs = tokenizer([body], max_length=1024, return_tensors=\"pt\")\n",
        "\n",
        "    # Generate Summary\n",
        "    summary_ids = model.generate(inputs[\"input_ids\"], num_beams=4, min_length=100, max_length=240, no_repeat_ngram_size=5)\n",
        "    summarystr = tokenizer.batch_decode(summary_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\n",
        "    return summarystr\n",
        "\n",
        "dfCluster['summarytxt'] = dfCluster.text.apply(allsummary)\n",
        "display(dfCluster)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "inputWidgets": {},
          "nuid": "77cb5c99-c1d1-4f26-b316-1c1c76ac8ea4",
          "showTitle": false,
          "title": ""
        },
        "id": "BCcWx8HAdXB2"
      },
      "outputs": [],
      "source": [
        "# write to csv\n",
        "recommd_cluster.to_csv('CMV_Reports_Article_Recommendations_df.csv')\n",
        "dfCluster.to_csv('CMV_Reports_Article_Recommendations_dfCluster.csv')"
      ]
    }
  ],
  "metadata": {
    "application/vnd.databricks.v1+notebook": {
      "dashboards": [],
      "language": "python",
      "notebookMetadata": {
        "pythonIndentUnit": 4
      },
      "notebookName": "02_Sentence_Clustering",
      "notebookOrigID": 677698861151334,
      "widgets": {}
    },
    "kernelspec": {
      "display_name": "env",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.9"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}